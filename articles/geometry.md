# The Factor Model as a Geometric Object

*Why the internal covariance geometry of indicators constrains nothing
about their external causal relationships.*

## 2.1 Variables as Vectors in a Hilbert Space

We begin with a standard geometric reformulation. Let \\(\Omega,
\mathcal{F}, P)\\ be a probability space. Every mean-zero,
finite-variance random variable \\X\\ can be identified with an element
of the Hilbert space \\L^2(\Omega, P)\\, where the inner product is
given by covariance:

\\ \langle X_i,\\ X_j \rangle \\=\\ \mathrm{Cov}(X_i, X_j). \tag{1} \\

The induced norm is \\\\X_i\\ = \sqrt{\mathrm{Var}(X_i)}\\, and the
cosine of the angle between two variable-vectors is exactly their
correlation:

\\ \cos\theta\_{ij} \\=\\ \frac{\langle X_i,\\
X_j\rangle}{\\X_i\\\\\\X_j\\} \\=\\ \mathrm{Corr}(X_i, X_j). \tag{2} \\

![\*\*Figure 1.\*\* Two variable-vectors in \$L^2\$. The angle
\$\theta\$ between them satisfies \$\cos\theta = \mathrm{Corr}(X_i,
X_j)\$. From left to right: high positive correlation (\$r = 0.9\$,
narrow angle), zero correlation (\$r = 0\$, orthogonal), and negative
correlation (\$r = -0.7\$, obtuse
angle).](geometry_files/figure-html/fig1-angle-correlation-1.png)

**Figure 1.** Two variable-vectors in \\L^2\\. The angle \\\theta\\
between them satisfies \\\cos\theta = \mathrm{Corr}(X_i, X_j)\\. From
left to right: high positive correlation (\\r = 0.9\\, narrow angle),
zero correlation (\\r = 0\\, orthogonal), and negative correlation (\\r
= -0.7\\, obtuse angle).

Uncorrelated variables correspond to orthogonal vectors (\\\theta\_{ij}
= 90°\\); perfectly correlated variables are collinear (\\\theta\_{ij} =
0°\\). This is the setting in which we will interpret the factor model.

## 2.2 The One-Factor Model as a Geometric Decomposition

The basic univariate latent factor model posits that each (centred)
indicator \\X_i\\, \\i = 1, \ldots, d\\, decomposes as:

\\ X_i \\=\\ \lambda_i\\\eta \\+\\ \varepsilon_i, \tag{3} \\

where \\\eta\\ is a latent factor with \\\mathrm{Var}(\eta) = 1\\, the
loading \\\lambda_i \neq 0\\, and the residual \\\varepsilon_i\\
satisfies \\\mathrm{Cov}(\eta, \varepsilon_i) = 0\\ and
\\\mathrm{Cov}(\varepsilon_i, \varepsilon_j) = 0\\ for \\i \neq j\\. In
\\L^2\\, equation (3) is an orthogonal decomposition of each
variable-vector into two components:

> **Geometric Reading of Eq. (3).** Each indicator \\X_i\\ is the sum of
> a *signal component* \\\lambda_i \eta\\, which lies along the
> one-dimensional subspace \\\mathcal{S} = \mathrm{span}\\\eta\\\\, and
> a *noise component* \\\varepsilon_i\\, which lies in a direction
> orthogonal to \\\mathcal{S}\\ and orthogonal to every other
> \\\varepsilon_j\\. The loading \\\lambda_i\\ is the signed length of
> the projection of \\X_i\\ onto \\\eta\\.

![\*\*Figure 2.\*\* Three indicator-vectors fanning from the latent axis
\$\eta\$. Each \$X_i\$ decomposes into a signal projection
\$\lambda_i\eta\$ along the axis (dashed) and a noise residual
\$\varepsilon_i\$ perpendicular to it. The fan shape --- all indicators
clustered around one axis --- is the geometric signature of a one-factor
model.](geometry_files/figure-html/fig2-fan-decomposition-1.png)

**Figure 2.** Three indicator-vectors fanning from the latent axis
\\\eta\\. Each \\X_i\\ decomposes into a signal projection
\\\lambda_i\eta\\ along the axis (dashed) and a noise residual
\\\varepsilon_i\\ perpendicular to it. The fan shape — all indicators
clustered around one axis — is the geometric signature of a one-factor
model.

### Covariance as inner product

Because the decomposition in (3) is orthogonal, the covariance between
any two indicators reduces to the inner product of their signal
components alone:

\\ \mathrm{Cov}(X_i, X_j) \\=\\ \lambda_i\\\lambda_j. \tag{4} \\

The entire \\d \times d\\ covariance matrix has rank one, generated by
\\\mathbf{\lambda}\mathbf{\lambda}^\top\\. The internal geometry is
fully determined by the loadings. Fitting a one-factor model and finding
it adequate is equivalent to verifying that the indicator vectors
cluster tightly around a single axis in \\L^2\\.

## 2.3 The Structural Assumption as a Constraint on External Geometry

Now let \\Y\\ be any external variable — an outcome, a treatment, or an
arbitrary covariate — also viewed as a vector in \\L^2\\. The critical
observation is:

> **Key Insight.** The factor model (3) constrains only the *mutual*
> inner geometry of the vectors \\\\X_1, \ldots, X_d\\\\. It says
> **nothing** about the position of \\Y\\ in this space. The vector
> \\Y\\ can point in any direction whatsoever while the indicators
> retain the exact same fan-shaped configuration around \\\eta\\.

To see this algebraically, decompose any variable’s covariance with an
indicator:

\\ \mathrm{Cov}(X_i,\\ Y) \\=\\ \lambda_i\\\mathrm{Cov}(\eta,\\Y) \\+\\
\mathrm{Cov}(\varepsilon_i,\\Y). \tag{5} \\

The factor model imposes *no constraint* on
\\\mathrm{Cov}(\varepsilon_i, Y)\\. Two sharply different scenarios
yield identical internal geometry:

### Scenario A — Structural

If the structural interpretation holds, \\\mathrm{Cov}(\varepsilon_i, Y)
= 0\\ for all \\i\\, so:

\\ \mathrm{Cov}(X_i,\\ Y) \\=\\ \lambda_i\\\mathrm{Cov}(\eta,\\Y),
\qquad \forall\\ i. \tag{6} \\

The ratio of covariances across any two indicators equals the ratio of
loadings:

\\ \frac{\mathrm{Cov}(X_i,\\Y)}{\mathrm{Cov}(X_j,\\Y)} \\=\\
\frac{\lambda_i}{\lambda_j}, \qquad \forall\\ i,j. \tag{7} \\

### Scenario B — Non-structural

If some indicators have direct effects on \\Y\\, then some
\\\mathrm{Cov}(\varepsilon_i, Y) \neq 0\\, and equation (5) cannot be
simplified — proportionality breaks.

![\*\*Figure 3.\*\* The same indicator fan under two scenarios.
\*\*Structural (A):\*\* \$Y\$ projects only onto \$\eta\$; its
covariance with each \$X_i\$ is proportional to \$\lambda_i\$.
\*\*Non-structural (B):\*\* \$Y\$ also projects onto \$\varepsilon_3\$'s
direction, giving \$X_3\$ an extra association with \$Y\$ that breaks
proportionality --- even though the fan of indicators is
unchanged.](geometry_files/figure-html/fig3-structural-vs-non-1.png)

**Figure 3.** The same indicator fan under two scenarios. **Structural
(A):** \\Y\\ projects only onto \\\eta\\; its covariance with each
\\X_i\\ is proportional to \\\lambda_i\\. **Non-structural (B):** \\Y\\
also projects onto \\\varepsilon_3\\’s direction, giving \\X_3\\ an
extra association with \\Y\\ that breaks proportionality — even though
the fan of indicators is unchanged.

## 2.4 The Testable Implication, Geometrically

The structural assumption is equivalent to requiring that \\Y\\’s
projection onto the indicator subspace lies entirely in \\\mathcal{S} =
\mathrm{span}\\\eta\\\\:

> **Testable Geometric Constraint.** Under the structural
> interpretation, \\Y\\ must be orthogonal to every noise direction:
>
> \\\langle \varepsilon_i,\\ Y \rangle = 0, \qquad i = 1, \ldots, d.\\
>
> VanderWeele and Vansteelandt’s test checks whether the scaled
> associations \\E(X_i \mid Z{=}z) / \lambda_i\\ are constant across
> indicators \\i\\.

![\*\*Figure 4.\*\* The proportionality test in action. \*Left:\* under
a structural model, the raw covariances \$\mathrm{Cov}(X_i, Y)\$ differ
across indicators because loadings differ, but once divided by
\$\lambda_i\$ they collapse to a single constant (dashed line).
\*Right:\* under a non-structural model, the last indicator deviates
sharply after scaling, breaking the constant
line.](geometry_files/figure-html/fig4-proportionality-1.png)

**Figure 4.** The proportionality test in action. *Left:* under a
structural model, the raw covariances \\\mathrm{Cov}(X_i, Y)\\ differ
across indicators because loadings differ, but once divided by
\\\lambda_i\\ they collapse to a single constant (dashed line). *Right:*
under a non-structural model, the last indicator deviates sharply after
scaling, breaking the constant line.

When the test rejects — as it does for the Satisfaction with Life Scale
and all-cause mortality — it means the data reveal that \\Y\\’s vector
has non-negligible projections onto indicator-specific noise directions.
The internal fan is intact, but \\Y\\ does not align with its axis.
