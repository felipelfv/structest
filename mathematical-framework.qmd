---
title: "Mathematical Framework: Testing the Structural Interpretation of a Latent Factor Model"
subtitle: "A step-by-step derivation connecting VanderWeele & Vansteelandt (2022) to the structest package"
format:
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    theme: cosmo
    html-math-method: mathjax
---

## The Basic Latent Factor Model

### The Reflective Model

We observe $d$ indicators $X_1, \dots, X_d$ on each of $N$ subjects. The classical univariate latent factor model --- sometimes called a *reflective model* --- assumes these indicators are driven by a single unobserved latent variable $\eta$ (see Figure 1 of the paper). The causal direction runs *from* $\eta$ *to* the indicators: each $X_i$ is a noisy reflection of the latent variable. This is captured by:

$$
X_i = \mu_i + \lambda_i \eta + \varepsilon_i, \qquad i = 1, \dots, d \tag{1}
$$

with the following assumptions:

- $\varepsilon_i$ has mean zero: $E[\varepsilon_i] = 0$
- $\varepsilon_i$ is independent of $\eta$: $\varepsilon_i \perp \!\!\! \perp \eta$
- The errors are mutually independent: $\varepsilon_i \perp \!\!\! \perp \varepsilon_j$ for $i \neq j$
- Each $\lambda_i \neq 0$

Here $\mu_i$ is the intercept for indicator $i$, $\lambda_i$ is the *reliability* (how strongly $\eta$ influences $X_i$), and $\varepsilon_i$ is random error. No restrictions are placed on the mean or variance of $\eta$.

### Deriving the Covariance Structure

We now derive the covariance between any two indicators $X_i$ and $X_j$ (with $i \neq j$) from first principles, without imposing any constraints on $\eta$.

**Step 1.** Write out the covariance using Equation (1). The intercepts $\mu_i$ are constants, so they drop out:

$$
\text{Cov}(X_i, X_j) = \text{Cov}(\mu_i + \lambda_i \eta + \varepsilon_i, \; \mu_j + \lambda_j \eta + \varepsilon_j) = \text{Cov}(\lambda_i \eta + \varepsilon_i, \; \lambda_j \eta + \varepsilon_j)
$$

**Step 2.** Expand by bilinearity of covariance:

$$
= \lambda_i \lambda_j \, \text{Cov}(\eta, \eta) + \lambda_i \, \text{Cov}(\eta, \varepsilon_j) + \lambda_j \, \text{Cov}(\varepsilon_i, \eta) + \text{Cov}(\varepsilon_i, \varepsilon_j)
$$

**Step 3.** Apply the independence assumptions. Since $\varepsilon_j \perp \!\!\! \perp \eta$, we have $\text{Cov}(\eta, \varepsilon_j) = 0$. Since $\varepsilon_i \perp \!\!\! \perp \eta$, we have $\text{Cov}(\varepsilon_i, \eta) = 0$. Since $\varepsilon_i \perp \!\!\! \perp \varepsilon_j$ for $i \neq j$, we have $\text{Cov}(\varepsilon_i, \varepsilon_j) = 0$. This leaves:

$$
\text{Cov}(X_i, X_j) = \lambda_i \lambda_j \, \text{Var}(\eta) \qquad \text{for } i \neq j \tag{1a}
$$

This is the **general** covariance identity. It holds for any distribution of $\eta$.

### Identification and Normalisation

Equation (1a) reveals an identification problem: $\lambda_i$ and $\text{Var}(\eta)$ are not separately identifiable from the covariances alone. If we replace $\lambda_i$ with $\lambda_i^* = \lambda_i \sigma_\eta$ (where $\sigma_\eta = \sqrt{\text{Var}(\eta)}$) and $\eta$ with $\eta^* = \eta / \sigma_\eta$, then $\text{Var}(\eta^*) = 1$ and:

$$
\lambda_i \eta = (\lambda_i \sigma_\eta)(\eta / \sigma_\eta) = \lambda_i^* \eta^*
$$

The model is observationally identical. Similarly, if $E[\eta] = \mu_\eta \neq 0$, we can absorb the mean into the intercepts: $\mu_i^* = \mu_i + \lambda_i \mu_\eta$ and $\eta^* = \eta - \mu_\eta$.

To pin down a unique parameterisation, the paper adopts the **normalisation** $E[\eta] = 0$ and $\text{Var}(\eta) = 1$. These are conventions for identification, not substantive restrictions on the model --- any data-generating process can be reparameterised to satisfy them.

Under this normalisation, Equation (1a) simplifies to:

$$
\text{Cov}(X_i, X_j) = \lambda_i \lambda_j \qquad \text{for } i \neq j \tag{1b}
$$

and Equation (1) becomes $X_i = \lambda_i \eta + \varepsilon_i$ (with $\mu_i = 0$ after centring the indicators, as the paper assumes).

::: {.callout-note title="Why the normalisation does not affect the tests"}
Both T0 and T1 depend only on the **ratio** $\lambda_i / \lambda_1$, not on the individual $\lambda_i$. Under the general model (without normalisation), the GLM in @sec-loading-estimation would estimate $\lambda_i \sigma_\eta$ rather than $\lambda_i$, but the ratio $(\lambda_i \sigma_\eta) / (\lambda_1 \sigma_\eta) = \lambda_i / \lambda_1$ is invariant. So the normalisation is convenient but not necessary for the tests to work.
:::

We also get $\text{Var}(X_i) = \lambda_i^2 \,\text{Var}(\eta) + \text{Var}(\varepsilon_i)$, but we will not use this.

### Structural vs Basic: What the Measurement Model Cannot Tell Us

The basic latent factor model in Equation (1) specifies the joint distribution of $(X_1, \dots, X_d)$ through $\eta$ and $\varepsilon_i$, but it says nothing about the *causal* role of $\eta$ or the indicators with respect to any external variable. The same measurement model is consistent with very different causal structures. The paper illustrates this with two scenarios.

#### Scenario 1: $\eta$ causes an outcome $Y$ (Figure 2 of the paper)

Consider an external outcome $Y$. Two causal structures are both fully consistent with Equation (1):

- **(a) Structural interpretation:** $\eta \to X_i$ and $\eta \to Y$. The latent variable $\eta$ is causally efficacious for $Y$; the indicators $X_i$ are causally inert --- they merely reflect $\eta$.
- **(b) Basic (non-structural) interpretation:** $\eta \to X_i$ and $X_i \to Y$. Each indicator $X_i$ has its own direct causal effect on $Y$. The latent variable $\eta$ generates the indicators' covariance, but has no causal role of its own vis-à-vis $Y$.

#### Scenario 2: A treatment $T$ causes $\eta$ (Figure 3 of the paper)

The same ambiguity arises when we consider an external variable $T$ (e.g. a randomised treatment). Let $A = f(X_1, \dots, X_d)$ be some composite measure formed from the indicators. Two structures are again consistent with Equation (1):

- **(a) Structural:** $T \to \eta \to X_i \to A$. The treatment $T$ affects $\eta$, which in turn affects all indicators proportionally (through the $\lambda_i$). The composite $A$ reflects changes in $\eta$.
- **(b) Non-structural:** $T \to X_i$ directly, bypassing $\eta$. The treatment affects each indicator to differing degrees that have nothing to do with the $\lambda_i$.

#### Why the measurement model cannot distinguish these

Let $\Sigma_X$ denote the covariance matrix of $(X_1, \dots, X_d)$. Under both the structural and non-structural interpretations, $\Sigma_X$ has the same factor structure with off-diagonal entries $\lambda_i \lambda_j \,\text{Var}(\eta)$. No amount of analysis of $\Sigma_X$ alone can distinguish the two, because Equation (1) only constrains the joint distribution of the $X_i$ through $\eta$ --- it is silent about any external variable.

This is the paper's key point: **a well-fitting measurement model does not imply a structural interpretation**. To test whether the structural interpretation holds, we need an external variable $Z$ and additional data.

## Theorem 1: The Testable Implication

### The Role of $Z$

The variable $Z$ is *any* external variable. It could be:

- a **treatment** that causes $\eta$ (as in Scenario 2 above: $T \to \eta$),
- an **outcome** caused by $\eta$ (as in Scenario 1: $\eta \to Y$),
- or any other variable associated with $\eta$.

The *direction* of the causal arrow between $Z$ and $\eta$ does not matter for the test. What matters is the **conditional independence** assumption:

$$
Z \perp \!\!\! \perp (X_1, \dots, X_d) \mid \eta
$$

This says that, once we know $\eta$, learning $Z$ gives no additional information about the indicators. Equivalently: all paths between $Z$ and the indicators go through $\eta$. This is precisely what it means for the model to be *structural* --- $Z$ can only reach $X_i$ via $\eta$, regardless of whether $Z$ is upstream or downstream of $\eta$.

If the structural interpretation fails --- for instance if $Z$ directly affects some indicator $X_j$, bypassing $\eta$ --- then the conditional independence breaks down, and the testable implication below will be violated.

### Statement

> **Theorem 1.** Suppose that $Z$ is independent of $(X_1, \dots, X_d)$ conditional on $\eta$ and that the basic latent factor model in Equation (1) holds. Then for any $i$ and $j$, and any values $z$ and $z^*$:
>
> $$\lambda_i \left\{E(X_j \mid Z = z) - E(X_j \mid Z = z^*)\right\} = \lambda_j \left\{E(X_i \mid Z = z) - E(X_i \mid Z = z^*)\right\} \tag{T1}$$

### Proof

We derive the identity step by step.

**Step 1.** Compute $E(X_i \mid Z = z)$ using the law of iterated expectations. Since $Z$ is independent of $(X_1, \dots, X_d)$ conditional on $\eta$, we can write:

$$
E(X_i \mid Z = z) = E_\eta\!\left\{E(X_i \mid \eta, Z = z) \mid Z = z\right\}
$$

By the conditional independence assumption, $X_i \mid (\eta, Z) \overset{d}{=} X_i \mid \eta$, so:

$$
= E_\eta\!\left\{E(X_i \mid \eta) \mid Z = z\right\}
$$

**Step 2.** From Equation (1), $E(X_i \mid \eta) = \mu_i + \lambda_i \eta + E(\varepsilon_i) = \mu_i + \lambda_i \eta$ (since $\varepsilon_i$ has mean zero and is independent of $\eta$). Substituting:

$$
E(X_i \mid Z = z) = E_\eta\!\left\{\mu_i + \lambda_i \eta \mid Z = z\right\} = \mu_i + \lambda_i \, E(\eta \mid Z = z)
$$

**Step 3.** Similarly, for any other value $z^*$:

$$
E(X_i \mid Z = z^*) = \mu_i + \lambda_i \, E(\eta \mid Z = z^*)
$$

**Step 4.** Take the difference. The intercepts $\mu_i$ cancel:

$$
E(X_i \mid Z = z) - E(X_i \mid Z = z^*) = \lambda_i \left\{E(\eta \mid Z = z) - E(\eta \mid Z = z^*)\right\}
$$

Note that this holds regardless of $E[\eta]$ or $\text{Var}(\eta)$ --- the differencing eliminates both the intercept and the mean of $\eta$.

**Step 5.** Write the same identity for indicator $j$:

$$
E(X_j \mid Z = z) - E(X_j \mid Z = z^*) = \lambda_j \left\{E(\eta \mid Z = z) - E(\eta \mid Z = z^*)\right\}
$$

**Step 6.** Multiply the first by $\lambda_j$ and the second by $\lambda_i$:

$$
\lambda_j \left\{E(X_i \mid Z = z) - E(X_i \mid Z = z^*)\right\} = \lambda_i \lambda_j \left\{E(\eta \mid Z = z) - E(\eta \mid Z = z^*)\right\}
$$

$$
\lambda_i \left\{E(X_j \mid Z = z) - E(X_j \mid Z = z^*)\right\} = \lambda_i \lambda_j \left\{E(\eta \mid Z = z) - E(\eta \mid Z = z^*)\right\}
$$

The right-hand sides are identical. Therefore:

$$
\lambda_i \left\{E(X_j \mid Z = z) - E(X_j \mid Z = z^*)\right\} = \lambda_j \left\{E(X_i \mid Z = z) - E(X_i \mid Z = z^*)\right\} \qquad \blacksquare
$$

### Interpretation

The identity says that the "effect" of $Z$ on each indicator, when scaled by the reliability, must be the same. This is because under the structural model, $Z$ can only reach $X_i$ through $\eta$, and $\lambda_i$ governs how much of $\eta$'s variation is transmitted to $X_i$.

If $Z$ had a direct effect on some indicator $X_j$ that bypasses $\eta$ (as in the non-structural scenarios above), the effect on $X_j$ would not be proportional to $\lambda_j$ relative to the other indicators. The identity would fail, and the test would reject.

::: {.callout-tip title="Choosing Z in practice"}
The test has power only if $Z$ is actually associated with $\eta$ (i.e. $E(\eta \mid Z = z) - E(\eta \mid Z = z^*) \neq 0$ for some $z, z^*$). If $Z$ is independent of $\eta$, both sides of Equation (T1) are zero and the test is trivially satisfied regardless of whether the structural interpretation holds. In practice, $Z$ should be a variable that is expected to affect (or be affected by) the latent construct.
:::

## Estimating Factor Loadings (Section 3.1) {#sec-loading-estimation}

### From the Covariance Identity to Estimating Equations

From Equation (1b), under the normalisation $\text{Var}(\eta) = 1$, we have $d$ unknown loadings $\lambda_1, \dots, \lambda_d$ and $\binom{d}{2}$ observed pairwise covariances. We need to estimate $\lambda$ from data.

::: {.callout-note title="Without normalisation"}
Under the general model (1a) without normalisation, the GLM below would estimate $\tilde{\lambda}_i = \lambda_i \sigma_\eta$ rather than $\lambda_i$ itself. Since the tests only use the ratio $\tilde{\lambda}_i / \tilde{\lambda}_1 = \lambda_i / \lambda_1$, this makes no difference.
:::

Let $X_{ik}$ denote the value of indicator $i$ for subject $k = 1, \dots, N$. The sample mean of indicator $i$ is $\bar{X}_i = \frac{1}{N} \sum_{k=1}^{N} X_{ik}$. The sample covariance between indicators $i$ and $j$ is:

$$
C_{ij} = \frac{1}{N-1}\sum_{k=1}^{N}(X_{ik} - \bar{X}_i)(X_{jk} - \bar{X}_j)
$$

Under the normalised model, $E[C_{ij}] = \lambda_i \lambda_j$. A natural estimating equation for each indicator $i$ would set the observed covariances equal to their expected values. However, there are $\binom{d}{2}$ covariances and only $d$ unknowns, so we need to combine the equations.

The paper uses the following $d$ estimating equations (one per indicator):

$$
\sum_{\substack{j=1 \\ j \neq i}}^{d} \lambda_j \left(C_{ij} - \lambda_i \lambda_j\right) = 0 \qquad i = 1, \dots, d \tag{1c}
$$

**Where these come from:** Each equation sums over all pairs involving indicator $i$. The term $C_{ij} - \lambda_i \lambda_j$ is the residual for pair $(i,j)$: how far the observed covariance is from its expected value. The weight $\lambda_j$ comes from the derivative of $\lambda_i \lambda_j$ with respect to $\lambda_i$:

$$
\frac{\partial}{\partial \lambda_i}(\lambda_i \lambda_j) = \lambda_j
$$

So Equation (1c) is a weighted least-squares score equation. It has the form $\sum_j \frac{\partial f}{\partial \lambda_i} (C_{ij} - f_{ij}) = 0$ where $f_{ij} = \lambda_i \lambda_j$.

### Per-Subject Form

Equation (1c) uses summary statistics $C_{ij}$. For the variance adjustment in T0, we need the *per-subject* contributions. Writing $C_{ij} = \frac{1}{N-1}\sum_k (X_{ik} - \bar{X}_i)(X_{jk} - \bar{X}_j)$, we define the subject-level residual for pair $(i,j)$:

$$
u_{ij,k} = (X_{ik} - \bar{X}_i)(X_{jk} - \bar{X}_j) - \lambda_i \lambda_j
$$

This is subject $k$'s contribution to the covariance of pair $(i,j)$, minus the expected value under the model. Its sample mean is approximately $C_{ij} - \lambda_i \lambda_j$.

The per-subject loading score for indicator $i$ is then:

$$
V_{ik} = \sum_{\substack{j=1 \\ j \neq i}}^{d} \lambda_j \, u_{ij,k} = \sum_{\substack{j=1 \\ j \neq i}}^{d} \lambda_j \left\{(X_{ik} - \bar{X}_i)(X_{jk} - \bar{X}_j) - \lambda_i \lambda_j\right\} \tag{1d}
$$

This is the per-subject version of Equation (1c). Its sample mean is zero at the solution. The $N \times d$ matrix $V_k$ (with rows indexed by subjects and columns by indicators) will be used in the T0 variance adjustment.

### The Quasi-Poisson GLM (Computational Device)

Solving Equation (1c) directly is nonlinear. The paper observes that when all $\lambda_i > 0$ (which implies all $C_{ij} > 0$), we can take logarithms:

$$
\log E[C_{ij}] = \log(\lambda_i \lambda_j) = \log \lambda_i + \log \lambda_j
$$

Define $\Lambda_s = \log \lambda_s$. For each pair $(i,j)$, we have:

$$
\log E[C_{ij}] = \sum_{s=1}^{d} \Lambda_s \, I(s \in \{i,j\}) \tag{1e}
$$

where $I(s \in \{i,j\})$ is 1 if indicator $s$ is either $i$ or $j$, and 0 otherwise.

This is a generalised linear model with:

- **Response:** $C_{ij}$ (the $\binom{d}{2}$ sample covariances)
- **Link function:** $\log$ (so $\mu = \exp(B\Lambda)$)
- **Linear predictor:** $\Lambda_i + \Lambda_j$ (no intercept)
- **Design matrix** $B$: a $\binom{d}{2} \times d$ matrix where row $k$ (corresponding to pair $(i,j)$) has $B_{k,i} = 1$, $B_{k,j} = 1$, and all other entries 0
- **Family:** quasi-Poisson (we only use the mean-variance relationship $\text{Var} \propto \mu$, not the Poisson distribution)

The estimated loadings are $\hat{\lambda}_s = \exp(\hat{\Lambda}_s)$.

### Connection to `estimate_reliability()`

```{r}
#| eval: false
# Response: pairwise sample covariances
A[k] <- cov(X[, pairs[k, 1]], X[, pairs[k, 2]])

# Design matrix B: binom(d,2) x d
# B[k, s] = 1 if indicator s is in pair k
B <- build_design_matrix(d)

# Fit the GLM from Equation (1e)
fit <- glm(A ~ -1 + B, family = quasi(link = "log"))
lambda <- exp(coef(fit))
```

For $V_k$ (Equation 1c):

```{r}
#| eval: false
# Subject-level pair residuals: u_{ij,k}
U_pairs[, k] <- Xc[, i] * Xc[, j] - lambda[i] * lambda[j]

# Per-subject loading scores: V_{ik} = Σ_{j≠i} λ_j × u_{ij,k}
# For each pair (i1, i2) containing indicator i, add λ_{other} × u_{pair,k}
if (i1 == i) V_k[, i] <- V_k[, i] + lambda[i2] * U_pairs[, k]
if (i2 == i) V_k[, i] <- V_k[, i] + lambda[i1] * U_pairs[, k]
```


## The T0 Test: Reliability-Dependent (Section 3.2)

### From Theorem 1 to the Constrained Model

Consider a discrete variable $Z$ with levels $\{z_1, \dots, z_p\}$. Fix $z_1$ as the reference level and set $z^* = z_1$ in Theorem 1. For any indicator $i$ and level $z_j$:

$$
E(X_i \mid Z = z_j) - E(X_i \mid Z = z_1) = \lambda_i \left\{E(\eta \mid Z = z_j) - E(\eta \mid Z = z_1)\right\}
$$

(This follows from Step 4 of the Theorem 1 proof.)

Now define:

$$
\gamma_i \equiv E(X_i \mid Z = z_1) \qquad \beta_j \equiv E(X_1 \mid Z = z_j) - E(X_1 \mid Z = z_1) \tag{2a}
$$

Since $E(X_1 \mid Z = z_j) - E(X_1 \mid Z = z_1) = \lambda_1\{E(\eta \mid Z = z_j) - E(\eta \mid Z = z_1)\}$, we have:

$$
E(\eta \mid Z = z_j) - E(\eta \mid Z = z_1) = \frac{\beta_j}{\lambda_1}
$$

Substituting back:

$$
E(X_i \mid Z = z_j) = \gamma_i + \lambda_i \cdot \frac{\beta_j}{\lambda_1} = \gamma_i + \frac{\lambda_i}{\lambda_1}\,\beta_j \tag{2}
$$

Note that $\beta_1 = E(X_1 \mid Z = z_1) - E(X_1 \mid Z = z_1) = 0$ by construction.

The free parameters are $\gamma_1, \dots, \gamma_d$ ($d$ parameters) and $\beta_2, \dots, \beta_p$ ($p - 1$ parameters), totalling $d + p - 1$. The loadings $\lambda_i$ are treated as known (estimated separately).

### The Moment Conditions

Under the null hypothesis (Equation 2 holds), the residual for subject $k$, indicator $i$, and $Z$-level $j$ is:

$$
X_{ik} - \gamma_i - \frac{\lambda_i}{\lambda_1}\,\beta_j
$$

This residual should have mean zero among subjects with $Z_k = z_j$. We express this as a moment condition by multiplying by the indicator $I(Z_k = z_j)$:

$$
E\left[I(Z_k = z_j)\left(X_{ik} - \gamma_i - \frac{\lambda_i}{\lambda_1}\,\beta_j\right)\right] = 0
$$

**Why:** $E[I(Z_k = z_j) \cdot f(X_k)] = P(Z = z_j) \cdot E[f(X_k) \mid Z = z_j]$. So this moment condition is equivalent to $P(Z = z_j) \cdot \{E(X_i \mid Z = z_j) - \gamma_i - (\lambda_i/\lambda_1)\beta_j\} = 0$. Since $P(Z = z_j) > 0$, this holds if and only if the model is correct.

We have $d \times p$ such conditions (one for each indicator--level pair). Define the subject-level moment vector $U_k$ as a $(d \times p)$-dimensional column vector with elements:

$$
U_{(i,j),k} = I(Z_k = z_j)\left(X_{ik} - \gamma_i - \frac{\lambda_i}{\lambda_1}\,\beta_j\right), \quad i = 1,\dots,d, \;\; j = 1,\dots,p \tag{2b}
$$

### Counting Degrees of Freedom

We have $d \times p$ moment conditions and $d + p - 1$ free parameters. The number of overidentifying restrictions (degrees of freedom) is:

$$
dp - (d + p - 1) = dp - d - p + 1 = d(p-1) - (p-1) = (d-1)(p-1)
$$

For this to be positive (so the test has power), we need $d \geq 2$ and $p \geq 2$. But recall that estimating $\lambda$ requires $d \geq 3$, so T0 requires $d \geq 3$ and $p \geq 2$.

### The GMM Test Statistic

Define:

$$
\bar{g} = \frac{1}{N}\sum_{k=1}^{N} U_k
$$

This is the sample mean of the moment conditions --- a $(d \times p)$-dimensional vector. Under the null with correctly specified parameters, $\bar{g} \approx 0$.

Define:

$$
\Sigma = \frac{1}{N}\sum_{k=1}^{N}(U_k - \bar{U})(U_k - \bar{U})^\top
$$

the empirical covariance matrix of the $U_k$.

The GMM test statistic is:

$$
T_0 = N \, \bar{g}^\top \, \Sigma^{-1} \, \bar{g} \tag{2c}
$$

This is minimised over the free parameters $(\gamma, \beta)$ using numerical optimisation (`nlm`). By Theorem 9.2 of Newey & McFadden (1994), the minimised value converges to a $\chi^2_{(d-1)(p-1)}$ distribution under the null.

### Connection to `test_t0()` --- Moment Conditions

```{r}
#| eval: false
ratio <- lambda / lambda[1]   # λ_i / λ_1, fixed

q_t0 <- function(theta) {
  gamma <- theta[1:d]
  beta  <- c(0, theta[(d+1):(d+p-1)])   # β_1 = 0

  # Build U_k from Equation (2b)
  u <- matrix(0, nrow = n, ncol = d * p)
  col <- 0L
  for (i in 1:d) {
    for (j in 1:p) {
      col <- col + 1L
      pred <- gamma[i] + ratio[i] * beta[j]
      u[, col] <- Z_mat[, j] * (X[, i] - pred)
    }
  }
  g <- colMeans(u)   # ḡ from Equation (2c)
  # ... (variance adjustment follows)
}
```

### The Variance Adjustment

#### The Problem

::: {.callout-important title="Why the variance adjustment is necessary"}
In Equation (2), the ratios $\lambda_i / \lambda_1$ are treated as known constants. In practice, they are replaced by $\hat{\lambda}_i / \hat{\lambda}_1$ estimated from @sec-loading-estimation. This introduces additional sampling variability into $U_k$ that is not captured by $\text{Var}(U_k)$ alone.

If we ignore this, $\Sigma$ underestimates the true variability of $\bar{g}$, inflating $T_0$ and leading to too many false rejections.
:::

#### The General Principle

We have two sets of estimating equations evaluated on the same data:

- $V_k(\lambda)$: the loading equations (Equation 1c), which determine $\hat{\lambda}$
- $U_k(\gamma, \beta; \lambda)$: the structural test equations (Equation 2b), which depend on $\lambda$

The combined system is "stacked": we solve $V_k = 0$ for $\lambda$, plug $\hat{\lambda}$ into $U_k$, and solve $U_k = 0$ for $(\gamma, \beta)$.

From the theory of stacked estimating equations (see the Appendix of the paper, or Newey & McFadden, 1994), the adjusted influence function that accounts for the plug-in is:

$$
U^*_k = U_k - E\!\left[\frac{\partial U_k}{\partial \lambda}\right] \left(E\!\left[\frac{\partial V_k}{\partial \lambda}\right]\right)^{-1} V_k \tag{2d}
$$

**Intuition:** $V_k$ is the "noise" from estimating $\lambda$. The matrix $E[\partial U / \partial \lambda] \cdot (E[\partial V / \partial \lambda])^{-1}$ is the *sensitivity* of $U$ to perturbations in $\lambda$, normalised by how quickly $V$ responds to those same perturbations. The product tells us how much of $V_k$'s noise propagates into $U_k$. Subtracting it removes this propagated component.

The corrected covariance matrix is then $\Sigma^* = \frac{1}{N}\sum_k (U^*_k - \bar{U}^*)(U^*_k - \bar{U}^*)^\top$, and the test statistic uses $\Sigma^*$ instead of $\Sigma$. Crucially, the moment means $\bar{g}$ are still computed from the *original* $U_k$, not the adjusted $U^*_k$.

#### Deriving $E[\partial U / \partial \lambda]$

From Equation (2b), the moment condition for $(i,j)$ is:

$$
U_{(i,j),k} = I(Z_k = z_j)\!\left(X_{ik} - \gamma_i - \frac{\lambda_i}{\lambda_1}\beta_j\right)
$$

Only the term $\frac{\lambda_i}{\lambda_1}\beta_j$ depends on $\lambda$. Define $r_i = \lambda_i / \lambda_1$. The derivative of the prediction with respect to $\lambda_s$ is:

**Case $s = 1$ (denominator), $i > 1$, $j > 1$:**

$$
\frac{\partial}{\partial \lambda_1}\!\left(\frac{\lambda_i}{\lambda_1}\beta_j\right) = -\frac{\lambda_i}{\lambda_1^2}\,\beta_j = -\frac{r_i \, \beta_j}{\lambda_1}
$$

**Case $s = i$ (numerator), $i > 1$, $j > 1$:**

$$
\frac{\partial}{\partial \lambda_i}\!\left(\frac{\lambda_i}{\lambda_1}\beta_j\right) = \frac{\beta_j}{\lambda_1}
$$

**All other cases:** The derivative is zero. When $i = 1$, the ratio is $\lambda_1/\lambda_1 = 1$ (constant). When $j = 1$, $\beta_1 = 0$.

Since $\frac{\partial U_{(i,j),k}}{\partial \lambda_s} = -I(Z_k = z_j) \cdot \frac{\partial(\text{pred})}{\partial \lambda_s}$, taking expectations:

$$
E\!\left[\frac{\partial U_{(i,j)}}{\partial \lambda_s}\right] = -P(Z = z_j) \cdot \frac{\partial(\text{pred})}{\partial \lambda_s} \tag{2e}
$$

#### Deriving $E[\partial V / \partial \lambda]$

From Equation (1d):

$$
V_{ik} = \sum_{\substack{j=1 \\ j \neq i}}^{d} \lambda_j \left\{(X_{ik} - \bar{X}_i)(X_{jk} - \bar{X}_j) - \lambda_i \lambda_j\right\}
$$

**Diagonal ($\partial V_i / \partial \lambda_i$):** Each term in the sum contributes $\lambda_j \cdot (-\lambda_j) = -\lambda_j^2$:

$$
\frac{\partial V_i}{\partial \lambda_i} = -\sum_{\substack{j=1 \\ j \neq i}}^{d} \lambda_j^2 \tag{2f}
$$

**Off-diagonal ($\partial V_i / \partial \lambda_m$, $m \neq i$):** The $j = m$ term in the sum gives:

$$
\frac{\partial}{\partial \lambda_m}\!\left[\lambda_m\!\left\{(X_{ik} - \bar{X}_i)(X_{mk} - \bar{X}_m) - \lambda_i \lambda_m\right\}\right] = (X_{ik} - \bar{X}_i)(X_{mk} - \bar{X}_m) - 2\lambda_i\lambda_m
$$

Taking the sample average over $k$:

$$
\frac{1}{N}\sum_{k=1}^{N}\frac{\partial V_{ik}}{\partial \lambda_m} = C_{im} - 2\lambda_i\lambda_m = \underbrace{(C_{im} - \lambda_i\lambda_m)}_{\text{mean}(u_{im,k})} - \lambda_i\lambda_m \tag{2g}
$$

#### Connection to `test_t0()` --- Variance Adjustment

```{r}
#| eval: false
# Equation (2e): dpred/dλ, then dudlambda = -P(Z=z_j) × dpred/dλ
dpred_dlambda[col, 1] <- -ratio[i] * beta[j] / lambda[1]
dpred_dlambda[col, i] <- beta[j] / lambda[1]
dudlambda <- -diag(diag_pz) %*% dpred_dlambda

# Equations (2f) and (2g)
dvdlambda[i, i] <- -sum(lambda[-i]^2)
dvdlambda[i, k] <- mean_U_pairs[pair_idx] - lambda[i] * lambda[k]

# Equation (2d): the adjustment
uadjust <- u - t(dudlambda %*% solve(dvdlambda) %*% t(V_k))

# Equation (2c) with adjusted variance
Sigma <- var(uadjust) * ((n - 1) / n)   # 1/N normalisation
T0 <- n * drop(t(g) %*% solve(Sigma) %*% g)
```

Note that `g = colMeans(u)` uses the **original** (unadjusted) moments. Only the **variance** $\Sigma$ is computed from the adjusted $U^*_k$. The paper says to "re-define $\Sigma$ in the expression for $T_0$".

## Theorem 2 and the T1 Test: Reliability-Independent (Section 3.3)

### Motivation

::: {.callout-note title="Why a second test?"}
T0 depends on $\hat{\lambda}$, which requires the full distributional assumptions of Equation (1): independent errors, unit variance for $\eta$, etc. If the covariance structure of the $\varepsilon_i$ is misspecified, $\hat{\lambda}$ may be biased, and T0 may falsely reject the structural interpretation even when it holds. T1 avoids reliability estimation entirely, at the cost of requiring $p \geq 3$ levels of $Z$.
:::

### Theorem 2: Equivalent Reformulation

**Theorem 2.** Under the basic latent factor model in Equation (1), for any discrete $Z$, the following two conditions are equivalent:

1. For any $i$, $j$, and values $z$, $z^*$: $\;\lambda_i\{E(X_j \mid Z = z) - E(X_j \mid Z = z^*)\} = \lambda_j\{E(X_i \mid Z = z) - E(X_i \mid Z = z^*)\}$

2. For any $i$ and values $z$, and for an arbitrary fixed reference value $z^*$:
$$E(X_i \mid Z = z) - E(X_i \mid Z = z^*) = \alpha_i \, \beta_z \quad \text{for some parameters } \alpha_i, \beta_z$$

#### Proof that (1) implies (2)

**Step 1.** Let $\gamma_i = E(X_i \mid Z = z_1)$ where $z_1$ is the reference level. From Theorem 1 (Step 4 of the proof), we know:

$$
E(X_i \mid Z = z) - \gamma_i = \lambda_i\{E(\eta \mid Z = z) - E(\eta \mid Z = z_1)\}
$$

**Step 2.** Define:

$$
\alpha_i \equiv \frac{E(X_i \mid Z = z^*) - \gamma_i}{E(X_1 \mid Z = z^*) - \gamma_1} = \frac{\lambda_i\{E(\eta \mid Z = z^*) - E(\eta \mid Z = z_1)\}}{\lambda_1\{E(\eta \mid Z = z^*) - E(\eta \mid Z = z_1)\}} = \frac{\lambda_i}{\lambda_1}
$$

(provided $E(X_1 \mid Z = z^*) \neq \gamma_1$, i.e., $Z$ has some effect).

**Step 3.** Define $\beta_z \equiv E(X_1 \mid Z = z) - \gamma_1$. Then from Step 1:

$$
E(X_i \mid Z = z) - \gamma_i = \frac{\lambda_i}{\lambda_1} \cdot \lambda_1\{E(\eta \mid Z = z) - E(\eta \mid Z = z_1)\} = \alpha_i \, \beta_z
$$

So $E(X_i \mid Z = z) = \gamma_i + \alpha_i \, \beta_z$.  $\blacksquare$

#### What This Means

The matrix of mean differences:

$$
\Delta_{ij} = E(X_i \mid Z = z_j) - E(X_i \mid Z = z_1)
$$

is a $d \times p$ matrix (indicators $\times$ Z-levels). Under the structural model, it factors as:

$$
\Delta = \alpha \, \beta^\top
$$

where $\alpha = (\alpha_1, \dots, \alpha_d)^\top$ and $\beta = (\beta_1, \dots, \beta_p)^\top$. This is a **rank-1 matrix**. The T1 test checks whether $\Delta$ has rank $\leq 1$.

The key advantage: $\alpha_i$ and $\beta_j$ are free parameters estimated from data. We do not need to know $\lambda_i$.

### The Model Under the Null (Equation 3)

$$
E(X_i \mid Z = z_j) = \gamma_i + \alpha_i \, \beta_j \tag{3}
$$

with identification constraints $\alpha_1 = 1$ and $\beta_1 = 0$.

**Why $\alpha_1 = 1$:** The product $\alpha_i \beta_j$ is invariant to rescaling: $(\alpha_i / \tau)(\beta_j \tau) = \alpha_i \beta_j$ for any $\tau \neq 0$. Fixing $\alpha_1 = 1$ pins down the scale.

**Why $\beta_1 = 0$:** This sets $z_1$ as the reference level, so $E(X_i \mid Z = z_1) = \gamma_i$.

**Parameter count:**

- $\gamma_1, \dots, \gamma_d$: $d$ parameters
- $\alpha_2, \dots, \alpha_d$: $d - 1$ parameters (since $\alpha_1 = 1$)
- $\beta_2, \dots, \beta_p$: $p - 1$ parameters (since $\beta_1 = 0$)
- **Total:** $2d + p - 2$

### Counting Degrees of Freedom

Moment conditions: $d \times p$. Free parameters: $2d + p - 2$. Degrees of freedom:

$$
dp - (2d + p - 2) = dp - 2d - p + 2 = d(p - 2) - (p - 2) = (d-1)(p-2)
$$

For this to be positive, we need $d \geq 2$ and $p \geq 3$.

### The Moment Conditions

For each $(i, j)$:

$$
E\left[I(Z_k = z_j)\left(X_{ik} - \gamma_i - \alpha_i\,\beta_j\right)\right] = 0 \tag{3a}
$$

Define the subject-level moment vector $U_k$ with elements:

$$
U_{(i,j),k} = I(Z_k = z_j)\left(X_{ik} - \gamma_i - \alpha_i\,\beta_j\right)
$$

### Connection to `test_t1()`

```{r}
#| eval: false
build_gmm <- function(theta_val) {
  gam <- theta_val[1:d]
  alp <- c(1, theta_val[(d+1):(2*d-1)])     # α_1 = 1 fixed
  bet <- c(0, theta_val[(2*d):(2*d+p-2)])   # β_1 = 0 fixed

  # For each subject k, beta_k = β_{z_k}
  beta_k <- numeric(n)
  for (j in 1:p) beta_k[z == z_levels[j]] <- bet[j]

  # Residual matrix: X_{ik} - γ_i - α_i × β_{z_k}
  resid <- X - matrix(gam, n, d, byrow = TRUE) - outer(beta_k, alp)

  # Moment conditions from Equation (3a)
  g <- matrix(0, nrow = n, ncol = d * p)
  col <- 0L
  for (j in 1:p) {
    for (i in 1:d) {
      col <- col + 1L
      g[, col] <- Z_mat[, j] * resid[, i]
    }
  }
  g
}
```

### The Test Statistic and GMM Procedure

The T1 statistic has the same form as T0:

$$
T_1 = N \, \bar{g}^\top \, \Sigma^{-1} \, \bar{g} \tag{3b}
$$

minimised over $(\gamma, \alpha, \beta)$, converging to $\chi^2_{(d-1)(p-2)}$ under the null.

Unlike T0, there is no plug-in nuisance parameter, so **no variance adjustment is needed**.

However, Equation (3) is nonlinear in the parameters (the product $\alpha_i \beta_j$), so the optimisation requires care. The implementation uses a three-step procedure:

**Step 1 (Consistent estimates):** Use initial values from cell means. Form the weight matrix $W_1 = \Sigma_0^{-1}$ from initial residuals. Minimise $T_1$ with $W_1$ **fixed**. This gives consistent (but inefficient) estimates.

**Step 2 (Efficient estimates):** Recompute $W_2 = \Sigma_1^{-1}$ from step-1 residuals. Minimise $T_1$ with $W_2$ **fixed**. This is the standard two-step efficient GMM.

**Step 3 (Test statistic):** Minimise $T_1$ with $\Sigma$ **recomputed at each parameter value** (continuously-updating GMM). The minimised value is the test statistic.

```{r}
#| eval: false
# Step 1
Omega0 <- var(build_gmm(theta_init)) * ((n-1)/n)
W1 <- solve(Omega0)
res1 <- nlm(function(th) { gm <- colMeans(build_gmm(th)); n * crossprod(gm, W1 %*% gm) },
            p = theta_init)

# Step 2
Omega1 <- var(build_gmm(res1$estimate)) * ((n-1)/n)
W2 <- solve(Omega1)
res2 <- nlm(function(th) { gm <- colMeans(build_gmm(th)); n * crossprod(gm, W2 %*% gm) },
            p = res1$estimate)

# Step 3 (final, CU-GMM)
res <- nlm(function(th) {
  g <- build_gmm(th); gm <- colMeans(g)
  Sigma <- var(g) * ((n-1)/n)
  n * crossprod(gm, solve(Sigma) %*% gm)
}, p = res2$estimate)
```

### Initialisation from Cell Means

The initial parameter values come directly from the data:

**Step 1.** Compute cell means: $\hat{\mu}_{ij} = \frac{1}{n_j}\sum_{k: Z_k = z_j} X_{ik}$

**Step 2.** Set $\hat{\gamma}_i = \hat{\mu}_{i1}$ (reference-level means).

**Step 3.** Form the difference matrix $D_{ij} = \hat{\mu}_{ij} - \hat{\gamma}_i \approx \alpha_i \beta_j$.

**Step 4.** Since $\alpha_1 = 1$: $\beta_j \approx D_{1j}$.

**Step 5.** For $i \geq 2$, estimate $\alpha_i$ by least-squares regression of $D_{i,-1}$ on $\beta_{-1}$:

$$
\hat{\alpha}_i = \frac{\sum_{j=2}^{p} D_{ij} \, \hat{\beta}_j}{\sum_{j=2}^{p} \hat{\beta}_j^2}
$$

```{r}
#| eval: false
gamma_init <- cell_means[, 1]
D <- cell_means - gamma_init
beta_init <- D[1, ]
for (i in 2:d) {
  alpha_init[i] <- sum(D[i, -1] * beta_init[-1]) / sum(beta_init[-1]^2)
}
```

## Summary

$$
\boxed{X_i = \lambda_i \eta + \varepsilon_i} \quad \text{(Equation 1)}
$$

$$
\Downarrow \quad \text{Cov}(X_i, X_j) = \lambda_i\lambda_j
$$

$$
\texttt{estimate\_reliability()} \longrightarrow \hat{\lambda}
$$

$$
\Downarrow
$$

$$
\text{Theorem 1: } \lambda_i\{E(X_j|Z\!=\!z) - E(X_j|Z\!=\!z^*)\} = \lambda_j\{E(X_i|Z\!=\!z) - E(X_i|Z\!=\!z^*)\}
$$

$$
\swarrow \qquad\qquad\qquad\qquad \searrow
$$

| | **T0** (plug in $\hat{\lambda}$) | **T1** (reparameterise as rank-1) |
|---|---|---|
| **Equation** | $E(X_i \mid Z\!=\!z_j) = \gamma_i + \frac{\lambda_i}{\lambda_1}\beta_j$ | $E(X_i \mid Z\!=\!z_j) = \gamma_i + \alpha_i \beta_j$ |
| **Function** | `test_t0()` | `test_t1()` |
| **Statistic** | $T_0 \sim \chi^2_{(d-1)(p-1)}$ | $T_1 \sim \chi^2_{(d-1)(p-2)}$ |
| **Requires** | $d \geq 3$, $p \geq 2$ | $d \geq 2$, $p \geq 3$ |
| **Depends on Eq. (1)** | Yes (through $\hat{\lambda}$) | No |
| **Variance adjustment** | Yes (for uncertainty in $\hat{\lambda}$) | Not needed |

## References

VanderWeele, T. J. and Vansteelandt, S. (2022). A statistical test to reject the structural interpretation of a latent factor model. *Journal of the Royal Statistical Society: Series B (Statistical Methodology)*, **84**, 2032--2054. [doi:10.1111/rssb.12555](https://doi.org/10.1111/rssb.12555)

Newey, W. K. and McFadden, D. (1994). Large sample estimation and hypothesis testing. *Handbook of Econometrics*, **4**, 2111--2245.
